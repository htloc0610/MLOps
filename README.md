# üè† MLOps Pipeline - House Price Prediction (Local Version)

## üìã T·ªïng Quan

Pipeline ML ho√†n ch·ªânh ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√†, ch·∫°y **100% LOCAL** - kh√¥ng c·∫ßn Google Cloud Platform.

### ‚ú® T√≠nh NƒÉng

- ‚úÖ **Modular Architecture**: 4 components ƒë·ªôc l·∫≠p (Ingestion ‚Üí Preprocessing ‚Üí Training ‚Üí Evaluation)
- ‚úÖ **Local Execution**: Ch·∫°y tr·ª±c ti·∫øp tr√™n m√°y t√≠nh ho·∫∑c trong Docker
- ‚úÖ **No Cloud Dependencies**: Kh√¥ng c·∫ßn GCP, AWS, hay cloud n√†o kh√°c
- ‚úÖ **Comprehensive Metrics**: MSE, RMSE, MAE, R¬≤, MAPE
- ‚úÖ **Visualizations**: Plots t·ª± ƒë·ªông (Actual vs Predicted, Residuals, Feature Importance)
- ‚úÖ **Easy Configuration**: Environment variables qua `.env` file

---

## üöÄ Quick Start

### Option 1: Ch·∫°y Tr·ª±c Ti·∫øp v·ªõi Python

# 1. C√†i dependencies
pip install -r requirements.txt

# 2. ƒê·∫£m b·∫£o c√≥ dataset
# ƒê·∫∑t Housing.csv v√†o data/
# C√°ch 1: Docker Compose
docker-compose up

# C√°ch 2: Docker CLI
docker build -t mlops-pipeline .
docker run --rm -v %cd%/data:/app/data:ro -v %cd%/output:/app/output mlops-pipeline

---

## üìÅ C·∫•u Tr√∫c Project

```
MLOps/
‚îú‚îÄ‚îÄ üìÇ data/                    # Dataset directory
‚îÇ   ‚îî‚îÄ‚îÄ Housing.csv             # Input dataset
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/                     # Pipeline components (modular)
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py       # Load data
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py        # Clean, transform, split
‚îÇ   ‚îú‚îÄ‚îÄ training.py             # Train Random Forest
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py           # Evaluate & visualize
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py             # Package exports
‚îÇ
‚îú‚îÄ‚îÄ üìÇ output/                  # Results (auto-generated)
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model.pkl           # Trained model
‚îÇ   ‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics_*.json      # Evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ artifacts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl          # Feature scaler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictions.csv     # Predictions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.json
‚îÇ   ‚îî‚îÄ‚îÄ plots/
‚îÇ       ‚îî‚îÄ‚îÄ evaluation_plots.png # Visualizations
‚îÇ
‚îú‚îÄ‚îÄ üê≥ Dockerfile               # Docker image definition
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml       # Docker Compose config
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è .env.example             # Configuration template
‚îú‚îÄ‚îÄ üì¶ requirements.txt         # Python dependencies
‚îÇ
‚îî‚îÄ‚îÄ üìñ README.md                # This file
```

---

## ‚öôÔ∏è C·∫•u H√¨nh

### Environment Variables (`.env`)

```bash
# Model hyperparameters
N_ESTIMATORS=100        # Number of trees
MAX_DEPTH=10            # Tree depth
RANDOM_STATE=42         # Random seed

# Data split
TEST_SIZE=0.2           # 20% for testing
```

**T√πy ch·ªânh:**
```bash
# Train v·ªõi model m·∫°nh h∆°n
N_ESTIMATORS=200
MAX_DEPTH=15
```

---

## üìä Pipeline Workflow

```mermaid
graph LR
    A[Data Ingestion] --> B[Preprocessing]
    B --> C[Training]
    C --> D[Evaluation]
    
    A -->|Housing.csv| A1[Load & Validate]
    B -->|Clean Data| B1[Handle Missing]
    B1 --> B2[Encode Categories]
    B2 --> B3[Scale Features]
    B3 --> B4[Train/Test Split]
    C -->|Random Forest| C1[Fit Model]
    D -->|Metrics| D1[MSE, RMSE, MAE, R¬≤]
    D1 --> D2[Visualizations]
```

### Step-by-Step

**1. Data Ingestion**
- Load `Housing.csv` t·ª´ `data/`
- Validate dataset (shape, columns, missing values)

**2. Preprocessing**
- Fill missing values (median for numerics)
- Encode categorical variables (label encoding)
- Split train/test (80/20)
- Scale features (StandardScaler)

**3. Training**
- Train Random Forest Regressor
- Hyperparameters t·ª´ `.env`
- Log training metrics

**4. Evaluation**
- Calculate metrics: MSE, RMSE, MAE, R¬≤, MAPE
- Generate plots:
  - Actual vs Predicted
  - Residual plot
  - Feature importance
  - Residual distribution
- Save all results to `output/`

---

## üìà K·∫øt Qu·∫£ Output

### Metrics JSON
```json
{
  "mse": 1234.56,
  "rmse": 35.13,
  "mae": 25.42,
  "r2_score": 0.8523,
  "mape": 12.34
}
```

### Predictions CSV
```csv
actual,predicted,error,absolute_error,percentage_error
450000,445000,5000,5000,1.11
520000,518000,2000,2000,0.38
...
```

### Visualizations
- `evaluation_plots.png`: 4 plots in 1 figure
  - Actual vs Predicted scatter
  - Residual plot
  - Top 10 feature importance
  - Residual distribution

---

## üîß Advanced Usage

### 1. Customize Hyperparameters

**Via `.env`:**
```bash
N_ESTIMATORS=500
MAX_DEPTH=20
MIN_SAMPLES_SPLIT=5
```

**Via Docker:**
```bash
docker run -e N_ESTIMATORS=200 -e MAX_DEPTH=15 mlops-pipeline
```

### 2. Use as Python Module

```python
from src import data_ingestion, preprocessing, training, evaluation

# Load data
df = data_ingestion('data/Housing.csv')

# Preprocess
X_train, X_test, y_train, y_test, scaler = preprocessing(df)

# Train
model = training(X_train, y_train, {'n_estimators': 100, 'max_depth': 10})

# Evaluate
metrics = evaluation(model, X_test, y_test, output_dir='output/plots')
```

### 3. Load Trained Model

```python
import pickle

# Load model
with open('output/models/model.pkl', 'rb') as f:
    model = pickle.load(f)

# Load scaler
with open('output/artifacts/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Predict on new data
new_data_scaled = scaler.transform(new_data)
predictions = model.predict(new_data_scaled)
```

---

## üê≥ Docker

### Build Image
```bash
docker build -t mlops-pipeline:latest .
```

### Run Container
```bash
docker run --rm \
  -v %cd%/data:/app/data:ro \
  -v %cd%/output:/app/output \
  -e N_ESTIMATORS=200 \
  mlops-pipeline:latest
```

### Docker Compose
```bash
# Start
docker-compose up

# Run in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

Xem chi ti·∫øt: [DOCKER_GUIDE.md](DOCKER_GUIDE.md)

---

## üìö Dependencies

### Core ML
- `pandas` - Data manipulation
- `numpy` - Numerical computing
- `scikit-learn` - ML algorithms & metrics

### Visualization
- `matplotlib` - Plotting
- `seaborn` - Statistical visualizations

### Utilities
- `python-dotenv` - Environment variables

**T·ªïng dung l∆∞·ª£ng:** ~150MB (v·ªõi Docker image `python:3.9-slim`)

---

## ‚ùì Troubleshooting

### ‚ùå "Dataset not found"
```bash
# Gi·∫£i ph√°p 1: Download sample dataset
python download_dataset.py

# Gi·∫£i ph√°p 2: Copy file c·ªßa b·∫°n
mkdir data
copy path\to\Housing.csv data\
```

### ‚ùå "Module not found: src"
```bash
# ƒê·∫£m b·∫£o ch·∫°y t·ª´ project root
cd path\to\MLOps
python run_pipeline.py
```

### ‚ùå Docker errors
```bash
# Ki·ªÉm tra Docker ƒëang ch·∫°y
docker info

# Rebuild image (no cache)
docker-compose build --no-cache
```

### ‚ùå Low R¬≤ score
```bash
# TƒÉng complexity c·ªßa model
N_ESTIMATORS=200
MAX_DEPTH=15

# Ho·∫∑c check data quality (missing values, outliers)
```

---

## üîÑ Development Workflow

### 1. Modify Code
```bash
# S·ª≠a components trong src/
vi src/training.py

# Ho·∫∑c s·ª≠a main pipeline
vi run_pipeline.py
```

### 2. Test Locally
```bash
python run_pipeline.py
```

### 3. Rebuild Docker (n·∫øu c·∫ßn)
```bash
docker-compose build
docker-compose up
```

### 4. Check Results
```bash
# Xem metrics
cat output/metrics/metrics_*.json

# Xem plots
start output/plots/evaluation_plots.png
```

---

## üéØ Performance Tips

### 1. Faster Training
```bash
# Gi·∫£m s·ªë trees (trade-off: accuracy)
N_ESTIMATORS=50
MAX_DEPTH=8
```

### 2. Better Accuracy
```bash
# TƒÉng complexity
N_ESTIMATORS=200
MAX_DEPTH=15

# Fine-tune advanced params
MIN_SAMPLES_SPLIT=5
MIN_SAMPLES_LEAF=2
```

### 3. Memory Optimization
- S·ª≠ d·ª•ng smaller dataset cho testing
- Reduce `N_ESTIMATORS` n·∫øu RAM th·∫•p
- Close plots sau khi save trong evaluation

---

## üìñ Documentation

- **Main README**: This file
- **Docker Guide**: [DOCKER_GUIDE.md](DOCKER_GUIDE.md)
- **Setup Guide**: [SETUP.md](SETUP.md) (for GCP version - reference only)

---

## ‚úÖ Checklist

### Before Running
- [ ] Python 3.9+ installed (ho·∫∑c Docker Desktop)
- [ ] `Housing.csv` trong `data/`
- [ ] Dependencies installed (`pip install -r requirements.txt`)

### After Running
- [ ] Check `output/models/model.pkl` exists
- [ ] Review `output/metrics/*.json`
- [ ] View `output/plots/evaluation_plots.png`
- [ ] Check `output/artifacts/predictions.csv`

---

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

---

## üìÑ License

MIT License - feel free to use and modify

---

## üë• Authors

MLOps Team

---

**üöÄ Get Started:** `python run_pipeline.py` ho·∫∑c `run_docker.bat`
